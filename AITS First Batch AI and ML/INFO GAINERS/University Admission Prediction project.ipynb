{"nbformat_minor": 2, "cells": [{"source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 5}, {"source": "\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share your notebook.\nclient_e75c441252fc48c9891339a360d9277d = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='TXfdTI_4BnLjlZO9KWvV6NvNLhp-NHwfwT0t6OdApn2N',\n    ibm_auth_endpoint=\"https://iam.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n\nbody = client_e75c441252fc48c9891339a360d9277d.get_object(Bucket='universityadmissionprediction-donotdelete-pr-omi5zgdl4prmhk',Key='Admission_Predict_Ver1.1.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndataset = pd.read_csv(body)\ndataset.head()\n\n", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Serial No.</th>\n      <th>GRE Score</th>\n      <th>TOEFL Score</th>\n      <th>University Rating</th>\n      <th>SOP</th>\n      <th>LOR</th>\n      <th>CGPA</th>\n      <th>Research</th>\n      <th>Chance of Admit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>337</td>\n      <td>118</td>\n      <td>4</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>9.65</td>\n      <td>1</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>324</td>\n      <td>107</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>4.5</td>\n      <td>8.87</td>\n      <td>1</td>\n      <td>0.76</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>316</td>\n      <td>104</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>8.00</td>\n      <td>1</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>322</td>\n      <td>110</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>2.5</td>\n      <td>8.67</td>\n      <td>1</td>\n      <td>0.80</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>314</td>\n      <td>103</td>\n      <td>2</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>8.21</td>\n      <td>0</td>\n      <td>0.65</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n0           1        337          118                  4  4.5   4.5  9.65   \n1           2        324          107                  4  4.0   4.5  8.87   \n2           3        316          104                  3  3.0   3.5  8.00   \n3           4        322          110                  3  3.5   2.5  8.67   \n4           5        314          103                  2  2.0   3.0  8.21   \n\n   Research  Chance of Admit   \n0         1              1.00  \n1         1              0.76  \n2         1              0.72  \n3         1              0.80  \n4         0              0.65  "}, "execution_count": 6, "metadata": {}}], "execution_count": 6}, {"source": "dataset", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Serial No.</th>\n      <th>GRE Score</th>\n      <th>TOEFL Score</th>\n      <th>University Rating</th>\n      <th>SOP</th>\n      <th>LOR</th>\n      <th>CGPA</th>\n      <th>Research</th>\n      <th>Chance of Admit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>337</td>\n      <td>118</td>\n      <td>4</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>9.65</td>\n      <td>1</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>324</td>\n      <td>107</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>4.5</td>\n      <td>8.87</td>\n      <td>1</td>\n      <td>0.76</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>316</td>\n      <td>104</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>8.00</td>\n      <td>1</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>322</td>\n      <td>110</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>2.5</td>\n      <td>8.67</td>\n      <td>1</td>\n      <td>0.80</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>314</td>\n      <td>103</td>\n      <td>2</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>8.21</td>\n      <td>0</td>\n      <td>0.65</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>330</td>\n      <td>115</td>\n      <td>5</td>\n      <td>4.5</td>\n      <td>3.0</td>\n      <td>9.34</td>\n      <td>1</td>\n      <td>0.90</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>321</td>\n      <td>109</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>8.20</td>\n      <td>1</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>308</td>\n      <td>101</td>\n      <td>2</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>7.90</td>\n      <td>0</td>\n      <td>0.68</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>302</td>\n      <td>102</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>1.5</td>\n      <td>8.00</td>\n      <td>0</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>323</td>\n      <td>108</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>8.60</td>\n      <td>0</td>\n      <td>0.45</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>325</td>\n      <td>106</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>4.0</td>\n      <td>8.40</td>\n      <td>1</td>\n      <td>0.52</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>327</td>\n      <td>111</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>4.5</td>\n      <td>9.00</td>\n      <td>1</td>\n      <td>0.84</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>328</td>\n      <td>112</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>4.5</td>\n      <td>9.10</td>\n      <td>1</td>\n      <td>0.78</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>307</td>\n      <td>109</td>\n      <td>3</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>8.00</td>\n      <td>1</td>\n      <td>0.62</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>311</td>\n      <td>104</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>2.0</td>\n      <td>8.20</td>\n      <td>1</td>\n      <td>0.61</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>16</td>\n      <td>314</td>\n      <td>105</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>2.5</td>\n      <td>8.30</td>\n      <td>0</td>\n      <td>0.54</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>17</td>\n      <td>317</td>\n      <td>107</td>\n      <td>3</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>8.70</td>\n      <td>0</td>\n      <td>0.66</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>18</td>\n      <td>319</td>\n      <td>106</td>\n      <td>3</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>8.00</td>\n      <td>1</td>\n      <td>0.65</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>19</td>\n      <td>318</td>\n      <td>110</td>\n      <td>3</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>8.80</td>\n      <td>0</td>\n      <td>0.63</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>20</td>\n      <td>303</td>\n      <td>102</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>8.50</td>\n      <td>0</td>\n      <td>0.62</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>21</td>\n      <td>312</td>\n      <td>107</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>7.90</td>\n      <td>1</td>\n      <td>0.64</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>22</td>\n      <td>325</td>\n      <td>114</td>\n      <td>4</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>8.40</td>\n      <td>0</td>\n      <td>0.70</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>23</td>\n      <td>328</td>\n      <td>116</td>\n      <td>5</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>9.50</td>\n      <td>1</td>\n      <td>0.94</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>24</td>\n      <td>334</td>\n      <td>119</td>\n      <td>5</td>\n      <td>5.0</td>\n      <td>4.5</td>\n      <td>9.70</td>\n      <td>1</td>\n      <td>0.95</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>25</td>\n      <td>336</td>\n      <td>119</td>\n      <td>5</td>\n      <td>4.0</td>\n      <td>3.5</td>\n      <td>9.80</td>\n      <td>1</td>\n      <td>0.97</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>26</td>\n      <td>340</td>\n      <td>120</td>\n      <td>5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>9.60</td>\n      <td>1</td>\n      <td>0.94</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>27</td>\n      <td>322</td>\n      <td>109</td>\n      <td>5</td>\n      <td>4.5</td>\n      <td>3.5</td>\n      <td>8.80</td>\n      <td>0</td>\n      <td>0.76</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>28</td>\n      <td>298</td>\n      <td>98</td>\n      <td>2</td>\n      <td>1.5</td>\n      <td>2.5</td>\n      <td>7.50</td>\n      <td>1</td>\n      <td>0.44</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>29</td>\n      <td>295</td>\n      <td>93</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>7.20</td>\n      <td>0</td>\n      <td>0.46</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>30</td>\n      <td>310</td>\n      <td>99</td>\n      <td>2</td>\n      <td>1.5</td>\n      <td>2.0</td>\n      <td>7.30</td>\n      <td>0</td>\n      <td>0.54</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>470</th>\n      <td>471</td>\n      <td>320</td>\n      <td>110</td>\n      <td>5</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>9.27</td>\n      <td>1</td>\n      <td>0.87</td>\n    </tr>\n    <tr>\n      <th>471</th>\n      <td>472</td>\n      <td>311</td>\n      <td>103</td>\n      <td>3</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>8.09</td>\n      <td>0</td>\n      <td>0.64</td>\n    </tr>\n    <tr>\n      <th>472</th>\n      <td>473</td>\n      <td>327</td>\n      <td>116</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>4.5</td>\n      <td>9.48</td>\n      <td>1</td>\n      <td>0.90</td>\n    </tr>\n    <tr>\n      <th>473</th>\n      <td>474</td>\n      <td>316</td>\n      <td>102</td>\n      <td>2</td>\n      <td>4.0</td>\n      <td>3.5</td>\n      <td>8.15</td>\n      <td>0</td>\n      <td>0.67</td>\n    </tr>\n    <tr>\n      <th>474</th>\n      <td>475</td>\n      <td>308</td>\n      <td>105</td>\n      <td>4</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>7.95</td>\n      <td>1</td>\n      <td>0.67</td>\n    </tr>\n    <tr>\n      <th>475</th>\n      <td>476</td>\n      <td>300</td>\n      <td>101</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>2.5</td>\n      <td>7.88</td>\n      <td>0</td>\n      <td>0.59</td>\n    </tr>\n    <tr>\n      <th>476</th>\n      <td>477</td>\n      <td>304</td>\n      <td>104</td>\n      <td>3</td>\n      <td>2.5</td>\n      <td>2.0</td>\n      <td>8.12</td>\n      <td>0</td>\n      <td>0.62</td>\n    </tr>\n    <tr>\n      <th>477</th>\n      <td>478</td>\n      <td>309</td>\n      <td>105</td>\n      <td>4</td>\n      <td>3.5</td>\n      <td>2.0</td>\n      <td>8.18</td>\n      <td>0</td>\n      <td>0.65</td>\n    </tr>\n    <tr>\n      <th>478</th>\n      <td>479</td>\n      <td>318</td>\n      <td>103</td>\n      <td>3</td>\n      <td>4.0</td>\n      <td>4.5</td>\n      <td>8.49</td>\n      <td>1</td>\n      <td>0.71</td>\n    </tr>\n    <tr>\n      <th>479</th>\n      <td>480</td>\n      <td>325</td>\n      <td>110</td>\n      <td>4</td>\n      <td>4.5</td>\n      <td>4.0</td>\n      <td>8.96</td>\n      <td>1</td>\n      <td>0.79</td>\n    </tr>\n    <tr>\n      <th>480</th>\n      <td>481</td>\n      <td>321</td>\n      <td>102</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>4.0</td>\n      <td>9.01</td>\n      <td>1</td>\n      <td>0.80</td>\n    </tr>\n    <tr>\n      <th>481</th>\n      <td>482</td>\n      <td>323</td>\n      <td>107</td>\n      <td>4</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>8.48</td>\n      <td>1</td>\n      <td>0.78</td>\n    </tr>\n    <tr>\n      <th>482</th>\n      <td>483</td>\n      <td>328</td>\n      <td>113</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>2.5</td>\n      <td>8.77</td>\n      <td>1</td>\n      <td>0.83</td>\n    </tr>\n    <tr>\n      <th>483</th>\n      <td>484</td>\n      <td>304</td>\n      <td>103</td>\n      <td>5</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>7.92</td>\n      <td>0</td>\n      <td>0.71</td>\n    </tr>\n    <tr>\n      <th>484</th>\n      <td>485</td>\n      <td>317</td>\n      <td>106</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>7.89</td>\n      <td>1</td>\n      <td>0.73</td>\n    </tr>\n    <tr>\n      <th>485</th>\n      <td>486</td>\n      <td>311</td>\n      <td>101</td>\n      <td>2</td>\n      <td>2.5</td>\n      <td>3.5</td>\n      <td>8.34</td>\n      <td>1</td>\n      <td>0.70</td>\n    </tr>\n    <tr>\n      <th>486</th>\n      <td>487</td>\n      <td>319</td>\n      <td>102</td>\n      <td>3</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>8.37</td>\n      <td>0</td>\n      <td>0.68</td>\n    </tr>\n    <tr>\n      <th>487</th>\n      <td>488</td>\n      <td>327</td>\n      <td>115</td>\n      <td>4</td>\n      <td>3.5</td>\n      <td>4.0</td>\n      <td>9.14</td>\n      <td>0</td>\n      <td>0.79</td>\n    </tr>\n    <tr>\n      <th>488</th>\n      <td>489</td>\n      <td>322</td>\n      <td>112</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>8.62</td>\n      <td>1</td>\n      <td>0.76</td>\n    </tr>\n    <tr>\n      <th>489</th>\n      <td>490</td>\n      <td>302</td>\n      <td>110</td>\n      <td>3</td>\n      <td>4.0</td>\n      <td>4.5</td>\n      <td>8.50</td>\n      <td>0</td>\n      <td>0.65</td>\n    </tr>\n    <tr>\n      <th>490</th>\n      <td>491</td>\n      <td>307</td>\n      <td>105</td>\n      <td>2</td>\n      <td>2.5</td>\n      <td>4.5</td>\n      <td>8.12</td>\n      <td>1</td>\n      <td>0.67</td>\n    </tr>\n    <tr>\n      <th>491</th>\n      <td>492</td>\n      <td>297</td>\n      <td>99</td>\n      <td>4</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>7.81</td>\n      <td>0</td>\n      <td>0.54</td>\n    </tr>\n    <tr>\n      <th>492</th>\n      <td>493</td>\n      <td>298</td>\n      <td>101</td>\n      <td>4</td>\n      <td>2.5</td>\n      <td>4.5</td>\n      <td>7.69</td>\n      <td>1</td>\n      <td>0.53</td>\n    </tr>\n    <tr>\n      <th>493</th>\n      <td>494</td>\n      <td>300</td>\n      <td>95</td>\n      <td>2</td>\n      <td>3.0</td>\n      <td>1.5</td>\n      <td>8.22</td>\n      <td>1</td>\n      <td>0.62</td>\n    </tr>\n    <tr>\n      <th>494</th>\n      <td>495</td>\n      <td>301</td>\n      <td>99</td>\n      <td>3</td>\n      <td>2.5</td>\n      <td>2.0</td>\n      <td>8.45</td>\n      <td>1</td>\n      <td>0.68</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>496</td>\n      <td>332</td>\n      <td>108</td>\n      <td>5</td>\n      <td>4.5</td>\n      <td>4.0</td>\n      <td>9.02</td>\n      <td>1</td>\n      <td>0.87</td>\n    </tr>\n    <tr>\n      <th>496</th>\n      <td>497</td>\n      <td>337</td>\n      <td>117</td>\n      <td>5</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>9.87</td>\n      <td>1</td>\n      <td>0.96</td>\n    </tr>\n    <tr>\n      <th>497</th>\n      <td>498</td>\n      <td>330</td>\n      <td>120</td>\n      <td>5</td>\n      <td>4.5</td>\n      <td>5.0</td>\n      <td>9.56</td>\n      <td>1</td>\n      <td>0.93</td>\n    </tr>\n    <tr>\n      <th>498</th>\n      <td>499</td>\n      <td>312</td>\n      <td>103</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>8.43</td>\n      <td>0</td>\n      <td>0.73</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>500</td>\n      <td>327</td>\n      <td>113</td>\n      <td>4</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>9.04</td>\n      <td>0</td>\n      <td>0.84</td>\n    </tr>\n  </tbody>\n</table>\n<p>500 rows \u00d7 9 columns</p>\n</div>", "text/plain": "     Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n0             1        337          118                  4  4.5   4.5  9.65   \n1             2        324          107                  4  4.0   4.5  8.87   \n2             3        316          104                  3  3.0   3.5  8.00   \n3             4        322          110                  3  3.5   2.5  8.67   \n4             5        314          103                  2  2.0   3.0  8.21   \n5             6        330          115                  5  4.5   3.0  9.34   \n6             7        321          109                  3  3.0   4.0  8.20   \n7             8        308          101                  2  3.0   4.0  7.90   \n8             9        302          102                  1  2.0   1.5  8.00   \n9            10        323          108                  3  3.5   3.0  8.60   \n10           11        325          106                  3  3.5   4.0  8.40   \n11           12        327          111                  4  4.0   4.5  9.00   \n12           13        328          112                  4  4.0   4.5  9.10   \n13           14        307          109                  3  4.0   3.0  8.00   \n14           15        311          104                  3  3.5   2.0  8.20   \n15           16        314          105                  3  3.5   2.5  8.30   \n16           17        317          107                  3  4.0   3.0  8.70   \n17           18        319          106                  3  4.0   3.0  8.00   \n18           19        318          110                  3  4.0   3.0  8.80   \n19           20        303          102                  3  3.5   3.0  8.50   \n20           21        312          107                  3  3.0   2.0  7.90   \n21           22        325          114                  4  3.0   2.0  8.40   \n22           23        328          116                  5  5.0   5.0  9.50   \n23           24        334          119                  5  5.0   4.5  9.70   \n24           25        336          119                  5  4.0   3.5  9.80   \n25           26        340          120                  5  4.5   4.5  9.60   \n26           27        322          109                  5  4.5   3.5  8.80   \n27           28        298           98                  2  1.5   2.5  7.50   \n28           29        295           93                  1  2.0   2.0  7.20   \n29           30        310           99                  2  1.5   2.0  7.30   \n..          ...        ...          ...                ...  ...   ...   ...   \n470         471        320          110                  5  4.0   4.0  9.27   \n471         472        311          103                  3  2.0   4.0  8.09   \n472         473        327          116                  4  4.0   4.5  9.48   \n473         474        316          102                  2  4.0   3.5  8.15   \n474         475        308          105                  4  3.0   2.5  7.95   \n475         476        300          101                  3  3.5   2.5  7.88   \n476         477        304          104                  3  2.5   2.0  8.12   \n477         478        309          105                  4  3.5   2.0  8.18   \n478         479        318          103                  3  4.0   4.5  8.49   \n479         480        325          110                  4  4.5   4.0  8.96   \n480         481        321          102                  3  3.5   4.0  9.01   \n481         482        323          107                  4  3.0   2.5  8.48   \n482         483        328          113                  4  4.0   2.5  8.77   \n483         484        304          103                  5  5.0   3.0  7.92   \n484         485        317          106                  3  3.5   3.0  7.89   \n485         486        311          101                  2  2.5   3.5  8.34   \n486         487        319          102                  3  2.5   2.5  8.37   \n487         488        327          115                  4  3.5   4.0  9.14   \n488         489        322          112                  3  3.0   4.0  8.62   \n489         490        302          110                  3  4.0   4.5  8.50   \n490         491        307          105                  2  2.5   4.5  8.12   \n491         492        297           99                  4  3.0   3.5  7.81   \n492         493        298          101                  4  2.5   4.5  7.69   \n493         494        300           95                  2  3.0   1.5  8.22   \n494         495        301           99                  3  2.5   2.0  8.45   \n495         496        332          108                  5  4.5   4.0  9.02   \n496         497        337          117                  5  5.0   5.0  9.87   \n497         498        330          120                  5  4.5   5.0  9.56   \n498         499        312          103                  4  4.0   5.0  8.43   \n499         500        327          113                  4  4.5   4.5  9.04   \n\n     Research  Chance of Admit   \n0           1              1.00  \n1           1              0.76  \n2           1              0.72  \n3           1              0.80  \n4           0              0.65  \n5           1              0.90  \n6           1              0.75  \n7           0              0.68  \n8           0              0.50  \n9           0              0.45  \n10          1              0.52  \n11          1              0.84  \n12          1              0.78  \n13          1              0.62  \n14          1              0.61  \n15          0              0.54  \n16          0              0.66  \n17          1              0.65  \n18          0              0.63  \n19          0              0.62  \n20          1              0.64  \n21          0              0.70  \n22          1              0.94  \n23          1              0.95  \n24          1              0.97  \n25          1              0.94  \n26          0              0.76  \n27          1              0.44  \n28          0              0.46  \n29          0              0.54  \n..        ...               ...  \n470         1              0.87  \n471         0              0.64  \n472         1              0.90  \n473         0              0.67  \n474         1              0.67  \n475         0              0.59  \n476         0              0.62  \n477         0              0.65  \n478         1              0.71  \n479         1              0.79  \n480         1              0.80  \n481         1              0.78  \n482         1              0.83  \n483         0              0.71  \n484         1              0.73  \n485         1              0.70  \n486         0              0.68  \n487         0              0.79  \n488         1              0.76  \n489         0              0.65  \n490         1              0.67  \n491         0              0.54  \n492         1              0.53  \n493         1              0.62  \n494         1              0.68  \n495         1              0.87  \n496         1              0.96  \n497         1              0.93  \n498         0              0.73  \n499         0              0.84  \n\n[500 rows x 9 columns]"}, "execution_count": 7, "metadata": {}}], "execution_count": 7}, {"source": "X = dataset.iloc[:, 1:8].values\nX", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[ 337.  ,  118.  ,    4.  , ...,    4.5 ,    9.65,    1.  ],\n       [ 324.  ,  107.  ,    4.  , ...,    4.5 ,    8.87,    1.  ],\n       [ 316.  ,  104.  ,    3.  , ...,    3.5 ,    8.  ,    1.  ],\n       ..., \n       [ 330.  ,  120.  ,    5.  , ...,    5.  ,    9.56,    1.  ],\n       [ 312.  ,  103.  ,    4.  , ...,    5.  ,    8.43,    0.  ],\n       [ 327.  ,  113.  ,    4.  , ...,    4.5 ,    9.04,    0.  ]])"}, "execution_count": 8, "metadata": {}}], "execution_count": 8}, {"source": "y= dataset.iloc[:, [8]].values", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 9}, {"source": "y", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[ 1.  ],\n       [ 0.76],\n       [ 0.72],\n       [ 0.8 ],\n       [ 0.65],\n       [ 0.9 ],\n       [ 0.75],\n       [ 0.68],\n       [ 0.5 ],\n       [ 0.45],\n       [ 0.52],\n       [ 0.84],\n       [ 0.78],\n       [ 0.62],\n       [ 0.61],\n       [ 0.54],\n       [ 0.66],\n       [ 0.65],\n       [ 0.63],\n       [ 0.62],\n       [ 0.64],\n       [ 0.7 ],\n       [ 0.94],\n       [ 0.95],\n       [ 0.97],\n       [ 0.94],\n       [ 0.76],\n       [ 0.44],\n       [ 0.46],\n       [ 0.54],\n       [ 0.65],\n       [ 0.74],\n       [ 0.91],\n       [ 0.9 ],\n       [ 0.94],\n       [ 0.88],\n       [ 0.64],\n       [ 0.58],\n       [ 0.52],\n       [ 0.48],\n       [ 0.46],\n       [ 0.49],\n       [ 0.53],\n       [ 0.87],\n       [ 0.91],\n       [ 0.88],\n       [ 0.86],\n       [ 0.89],\n       [ 0.82],\n       [ 0.78],\n       [ 0.76],\n       [ 0.56],\n       [ 0.78],\n       [ 0.72],\n       [ 0.7 ],\n       [ 0.64],\n       [ 0.64],\n       [ 0.46],\n       [ 0.36],\n       [ 0.42],\n       [ 0.48],\n       [ 0.47],\n       [ 0.54],\n       [ 0.56],\n       [ 0.52],\n       [ 0.55],\n       [ 0.61],\n       [ 0.57],\n       [ 0.68],\n       [ 0.78],\n       [ 0.94],\n       [ 0.96],\n       [ 0.93],\n       [ 0.84],\n       [ 0.74],\n       [ 0.72],\n       [ 0.74],\n       [ 0.64],\n       [ 0.44],\n       [ 0.46],\n       [ 0.5 ],\n       [ 0.96],\n       [ 0.92],\n       [ 0.92],\n       [ 0.94],\n       [ 0.76],\n       [ 0.72],\n       [ 0.66],\n       [ 0.64],\n       [ 0.74],\n       [ 0.64],\n       [ 0.38],\n       [ 0.34],\n       [ 0.44],\n       [ 0.36],\n       [ 0.42],\n       [ 0.48],\n       [ 0.86],\n       [ 0.9 ],\n       [ 0.79],\n       [ 0.71],\n       [ 0.64],\n       [ 0.62],\n       [ 0.57],\n       [ 0.74],\n       [ 0.69],\n       [ 0.87],\n       [ 0.91],\n       [ 0.93],\n       [ 0.68],\n       [ 0.61],\n       [ 0.69],\n       [ 0.62],\n       [ 0.72],\n       [ 0.59],\n       [ 0.66],\n       [ 0.56],\n       [ 0.45],\n       [ 0.47],\n       [ 0.71],\n       [ 0.94],\n       [ 0.94],\n       [ 0.57],\n       [ 0.61],\n       [ 0.57],\n       [ 0.64],\n       [ 0.85],\n       [ 0.78],\n       [ 0.84],\n       [ 0.92],\n       [ 0.96],\n       [ 0.77],\n       [ 0.71],\n       [ 0.79],\n       [ 0.89],\n       [ 0.82],\n       [ 0.76],\n       [ 0.71],\n       [ 0.8 ],\n       [ 0.78],\n       [ 0.84],\n       [ 0.9 ],\n       [ 0.92],\n       [ 0.97],\n       [ 0.8 ],\n       [ 0.81],\n       [ 0.75],\n       [ 0.83],\n       [ 0.96],\n       [ 0.79],\n       [ 0.93],\n       [ 0.94],\n       [ 0.86],\n       [ 0.79],\n       [ 0.8 ],\n       [ 0.77],\n       [ 0.7 ],\n       [ 0.65],\n       [ 0.61],\n       [ 0.52],\n       [ 0.57],\n       [ 0.53],\n       [ 0.67],\n       [ 0.68],\n       [ 0.81],\n       [ 0.78],\n       [ 0.65],\n       [ 0.64],\n       [ 0.64],\n       [ 0.65],\n       [ 0.68],\n       [ 0.89],\n       [ 0.86],\n       [ 0.89],\n       [ 0.87],\n       [ 0.85],\n       [ 0.9 ],\n       [ 0.82],\n       [ 0.72],\n       [ 0.73],\n       [ 0.71],\n       [ 0.71],\n       [ 0.68],\n       [ 0.75],\n       [ 0.72],\n       [ 0.89],\n       [ 0.84],\n       [ 0.93],\n       [ 0.93],\n       [ 0.88],\n       [ 0.9 ],\n       [ 0.87],\n       [ 0.86],\n       [ 0.94],\n       [ 0.77],\n       [ 0.78],\n       [ 0.73],\n       [ 0.73],\n       [ 0.7 ],\n       [ 0.72],\n       [ 0.73],\n       [ 0.72],\n       [ 0.97],\n       [ 0.97],\n       [ 0.69],\n       [ 0.57],\n       [ 0.63],\n       [ 0.66],\n       [ 0.64],\n       [ 0.68],\n       [ 0.79],\n       [ 0.82],\n       [ 0.95],\n       [ 0.96],\n       [ 0.94],\n       [ 0.93],\n       [ 0.91],\n       [ 0.85],\n       [ 0.84],\n       [ 0.74],\n       [ 0.76],\n       [ 0.75],\n       [ 0.76],\n       [ 0.71],\n       [ 0.67],\n       [ 0.61],\n       [ 0.63],\n       [ 0.64],\n       [ 0.71],\n       [ 0.82],\n       [ 0.73],\n       [ 0.74],\n       [ 0.69],\n       [ 0.64],\n       [ 0.91],\n       [ 0.88],\n       [ 0.85],\n       [ 0.86],\n       [ 0.7 ],\n       [ 0.59],\n       [ 0.6 ],\n       [ 0.65],\n       [ 0.7 ],\n       [ 0.76],\n       [ 0.63],\n       [ 0.81],\n       [ 0.72],\n       [ 0.71],\n       [ 0.8 ],\n       [ 0.77],\n       [ 0.74],\n       [ 0.7 ],\n       [ 0.71],\n       [ 0.93],\n       [ 0.85],\n       [ 0.79],\n       [ 0.76],\n       [ 0.78],\n       [ 0.77],\n       [ 0.9 ],\n       [ 0.87],\n       [ 0.71],\n       [ 0.7 ],\n       [ 0.7 ],\n       [ 0.75],\n       [ 0.71],\n       [ 0.72],\n       [ 0.73],\n       [ 0.83],\n       [ 0.77],\n       [ 0.72],\n       [ 0.54],\n       [ 0.49],\n       [ 0.52],\n       [ 0.58],\n       [ 0.78],\n       [ 0.89],\n       [ 0.7 ],\n       [ 0.66],\n       [ 0.67],\n       [ 0.68],\n       [ 0.8 ],\n       [ 0.81],\n       [ 0.8 ],\n       [ 0.94],\n       [ 0.93],\n       [ 0.92],\n       [ 0.89],\n       [ 0.82],\n       [ 0.79],\n       [ 0.58],\n       [ 0.56],\n       [ 0.56],\n       [ 0.64],\n       [ 0.61],\n       [ 0.68],\n       [ 0.76],\n       [ 0.86],\n       [ 0.9 ],\n       [ 0.71],\n       [ 0.62],\n       [ 0.66],\n       [ 0.65],\n       [ 0.73],\n       [ 0.62],\n       [ 0.74],\n       [ 0.79],\n       [ 0.8 ],\n       [ 0.69],\n       [ 0.7 ],\n       [ 0.76],\n       [ 0.84],\n       [ 0.78],\n       [ 0.67],\n       [ 0.66],\n       [ 0.65],\n       [ 0.54],\n       [ 0.58],\n       [ 0.79],\n       [ 0.8 ],\n       [ 0.75],\n       [ 0.73],\n       [ 0.72],\n       [ 0.62],\n       [ 0.67],\n       [ 0.81],\n       [ 0.63],\n       [ 0.69],\n       [ 0.8 ],\n       [ 0.43],\n       [ 0.8 ],\n       [ 0.73],\n       [ 0.75],\n       [ 0.71],\n       [ 0.73],\n       [ 0.83],\n       [ 0.72],\n       [ 0.94],\n       [ 0.81],\n       [ 0.81],\n       [ 0.75],\n       [ 0.79],\n       [ 0.58],\n       [ 0.59],\n       [ 0.47],\n       [ 0.49],\n       [ 0.47],\n       [ 0.42],\n       [ 0.57],\n       [ 0.62],\n       [ 0.74],\n       [ 0.73],\n       [ 0.64],\n       [ 0.63],\n       [ 0.59],\n       [ 0.73],\n       [ 0.79],\n       [ 0.68],\n       [ 0.7 ],\n       [ 0.81],\n       [ 0.85],\n       [ 0.93],\n       [ 0.91],\n       [ 0.69],\n       [ 0.77],\n       [ 0.86],\n       [ 0.74],\n       [ 0.57],\n       [ 0.51],\n       [ 0.67],\n       [ 0.72],\n       [ 0.89],\n       [ 0.95],\n       [ 0.79],\n       [ 0.39],\n       [ 0.38],\n       [ 0.34],\n       [ 0.47],\n       [ 0.56],\n       [ 0.71],\n       [ 0.78],\n       [ 0.73],\n       [ 0.82],\n       [ 0.62],\n       [ 0.96],\n       [ 0.96],\n       [ 0.46],\n       [ 0.53],\n       [ 0.49],\n       [ 0.76],\n       [ 0.64],\n       [ 0.71],\n       [ 0.84],\n       [ 0.77],\n       [ 0.89],\n       [ 0.82],\n       [ 0.84],\n       [ 0.91],\n       [ 0.67],\n       [ 0.95],\n       [ 0.63],\n       [ 0.66],\n       [ 0.78],\n       [ 0.91],\n       [ 0.62],\n       [ 0.52],\n       [ 0.61],\n       [ 0.58],\n       [ 0.57],\n       [ 0.61],\n       [ 0.54],\n       [ 0.56],\n       [ 0.59],\n       [ 0.49],\n       [ 0.72],\n       [ 0.76],\n       [ 0.65],\n       [ 0.52],\n       [ 0.6 ],\n       [ 0.58],\n       [ 0.42],\n       [ 0.77],\n       [ 0.73],\n       [ 0.94],\n       [ 0.91],\n       [ 0.92],\n       [ 0.71],\n       [ 0.71],\n       [ 0.69],\n       [ 0.95],\n       [ 0.74],\n       [ 0.73],\n       [ 0.86],\n       [ 0.71],\n       [ 0.64],\n       [ 0.55],\n       [ 0.58],\n       [ 0.61],\n       [ 0.67],\n       [ 0.66],\n       [ 0.53],\n       [ 0.79],\n       [ 0.92],\n       [ 0.87],\n       [ 0.92],\n       [ 0.91],\n       [ 0.93],\n       [ 0.84],\n       [ 0.8 ],\n       [ 0.79],\n       [ 0.82],\n       [ 0.89],\n       [ 0.93],\n       [ 0.73],\n       [ 0.71],\n       [ 0.59],\n       [ 0.51],\n       [ 0.37],\n       [ 0.69],\n       [ 0.89],\n       [ 0.77],\n       [ 0.68],\n       [ 0.62],\n       [ 0.57],\n       [ 0.45],\n       [ 0.54],\n       [ 0.71],\n       [ 0.78],\n       [ 0.81],\n       [ 0.86],\n       [ 0.87],\n       [ 0.64],\n       [ 0.9 ],\n       [ 0.67],\n       [ 0.67],\n       [ 0.59],\n       [ 0.62],\n       [ 0.65],\n       [ 0.71],\n       [ 0.79],\n       [ 0.8 ],\n       [ 0.78],\n       [ 0.83],\n       [ 0.71],\n       [ 0.73],\n       [ 0.7 ],\n       [ 0.68],\n       [ 0.79],\n       [ 0.76],\n       [ 0.65],\n       [ 0.67],\n       [ 0.54],\n       [ 0.53],\n       [ 0.62],\n       [ 0.68],\n       [ 0.87],\n       [ 0.96],\n       [ 0.93],\n       [ 0.73],\n       [ 0.84]])"}, "execution_count": 10, "metadata": {}}], "execution_count": 10}, {"source": "from sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state = 0)\nregressor.fit(X, y)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n           max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           presort=False, random_state=0, splitter='best')"}, "execution_count": 11, "metadata": {}}], "execution_count": 11}, {"source": "y_pred = regressor.predict([[380,60,2.5,2.5,2.5,5,1]])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 12}, {"source": "y_pred", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([ 0.7])"}, "execution_count": 13, "metadata": {}}], "execution_count": 13}, {"source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n2019-06-18 16:42:15,542 - watson_machine_learning_client.metanames - WARNING - 'AUTHOR_EMAIL' meta prop is deprecated. It will be ignored.\n"}], "execution_count": 14}, {"source": "wml_credentials={\n    \"access_key\":\"VPYh8brRRQAPBwoeRPa34MWiG4yyE5okIoHVOYlDDal2\",\n     \"instance_id\": \"885912a1-52ed-4ad8-97a6-5b6937ff9af0\",\n     \"password\": \"f77f8220-05c6-471c-aa5b-38ea87215cae\",\n    \"url\": \"https://eu-gb.ml.cloud.ibm.com\",\n    \"username\": \"e7d6393f-27f2-4ee8-865c-8ce01184fc49\"\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 15}, {"source": "client=WatsonMachineLearningAPIClient(wml_credentials)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 16}, {"source": "model_props={client.repository.ModelMetaNames.AUTHOR_NAME:\"IBM\",\n             client.repository.ModelMetaNames.AUTHOR_EMAIL:\"ibm@ibm.com\",\n             client.repository.ModelMetaNames.NAME:\"Decision Tree regression\"}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 17}, {"source": "model_artifact=client.repository.store_model(regressor,meta_props=model_props)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 18}, {"source": "published_model_uid=client.repository.get_model_uid(model_artifact)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 19}, {"source": "created_deployment=client.deployments.create(published_model_uid,name=\"Decision Tree\")", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: '2ab109bc-6f1b-491e-81d9-48dd82fa80da' started\n\n#######################################################################################\n\n\nINITIALIZING\nDEPLOY_SUCCESS\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='f45f265d-22b3-4de9-a3c1-6dd66f1adf0a'\n------------------------------------------------------------------------------------------------\n\n\n"}], "execution_count": 20}, {"source": "scoring_endpoint=client.deployments.get_scoring_url(created_deployment)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 21}, {"source": "scoring_endpoint", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "'https://eu-gb.ml.cloud.ibm.com/v3/wml_instances/885912a1-52ed-4ad8-97a6-5b6937ff9af0/deployments/f45f265d-22b3-4de9-a3c1-6dd66f1adf0a/online'"}, "execution_count": 22, "metadata": {}}], "execution_count": 22}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.5", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.5", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}